{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b02f70",
   "metadata": {},
   "source": [
    "# TD method on asset allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebb793",
   "metadata": {},
   "source": [
    "$Q^*_t(W_t,x_t)=-\\frac{K^{T-t-1}}{c}[p\\cdot e^{-c(a-r)(1+r)^{T-t-1}x_t}+(1-p)\\cdot e^{-c(b-r)(1+r)^{T-t-1}x_t}]\\cdot e^{-c(1+r)^{T-t}W_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05faca4a",
   "metadata": {},
   "source": [
    "$K = p\\cdot e^{-\\frac{a-r}{a-b}\\ln \\frac{(a-r)p}{(r-b)(1-p)}}+(1-p)\\cdot e^{-\\frac{b-r}{a-b}\\ln \\frac{(a-r)p}{(r-b)(1-p)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae710d",
   "metadata": {},
   "source": [
    "$x_t^*=\\frac{1}{c(a-b)(1+r)^{T-t-1}}\\cdot \\ln \\frac{(a-r)p}{(r-b)(1-p)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb3b71",
   "metadata": {},
   "source": [
    "**Critic:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896094d",
   "metadata": {},
   "source": [
    "Activation function: $-e^{-S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a63041",
   "metadata": {},
   "source": [
    "Feature functions:\n",
    "\n",
    "$\\phi_1((t,W_t,x_t)) = 1$\n",
    "\n",
    "$\\phi_2((t,W_t,x_t)) = T-t$\n",
    "\n",
    "$\\phi_3((t,W_t,x_t)) = (1+r)^{T-t}W_t$\n",
    "\n",
    "$\\phi_4((t,W_t,x_t)) = (1+r)^{T-t}x_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff674fc",
   "metadata": {},
   "source": [
    "**Actor:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37fc7b",
   "metadata": {},
   "source": [
    "Feature functions:\n",
    "\n",
    "$\\phi_1((t,W_t,x_t)) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978f188",
   "metadata": {},
   "source": [
    "$T = 10,\\ a = 0.18,\\ b = 0.02,\\ r = 0.10,\\ p = \\frac{2}{3},\\ c = 1,\\ W_0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df800b28",
   "metadata": {},
   "source": [
    "$Q^*_t(W_t,x_t)=-K^{T-t-1}[\\frac{2}{3}\\cdot e^{-0.08\\cdot 1.1^{T-t-1}x_t}+\\frac{1}{3}\\cdot e^{0.08\\cdot 1.1^{T-t-1}x_t}]\\cdot e^{-1.1^{T-t}W_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed20641",
   "metadata": {},
   "source": [
    "$K = \\frac{2}{3}\\cdot 2^{-\\frac{1}{2}}+\\frac{1}{3}\\cdot 2^{\\frac{1}{2}} \\approx 0.942809$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71fe6e",
   "metadata": {},
   "source": [
    "$x_t^*=\\frac{\\ln 2}{0.16\\cdot 1.1^{T-t-1}} \\approx \\frac{4.33217}{1.1^{T-t-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2606398",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2581d13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e53feec4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec043cb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbeb882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af983402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98108ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_state_dim, action_dim):\n",
    "        # 1 action state dim, 1 action dim\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # layer\n",
    "        self.layer_1 = nn.Linear(action_state_dim, action_dim, bias=False)\n",
    "        nn.init.constant_(self.layer_1.weight, 3)\n",
    "\n",
    "    def forward(self, s):\n",
    "        a = self.layer_1(s)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4095b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # 2 state dim, 1 action dim\n",
    "        super(Critic, self).__init__()\n",
    "        n_layer = 2\n",
    "        # layer\n",
    "        dim = state_dim + action_dim\n",
    "        self.layer_1 = nn.Linear(dim, n_layer, bias=True)\n",
    "        nn.init.normal_(self.layer_1.weight, 0., 0.1)\n",
    "        \n",
    "        self.output = nn.Linear(n_layer, 1, bias=False)\n",
    "        nn.init.normal_(self.layer_1.weight, 0.5, 0.1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        \n",
    "        s_a = torch.cat([s,a], dim=1)\n",
    "        s_a = self.layer_1(s_a)\n",
    "        s_a = -torch.exp(-s_a)\n",
    "        q_val = self.output(s_a)\n",
    "        return q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c554b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = torch.FloatTensor([1,2,3,4,5])\n",
    "aaa[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1237a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_state_dim, action_dim, replacement,memory_capacity=1000,gamma=1.0,lr_a=0.001, lr_c=0.002,batch_size=32) :\n",
    "        super(DDPG, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_state_dim = action_state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.replacement = replacement\n",
    "        self.t_replace_counter = 0\n",
    "        self.gamma = gamma\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # 记忆库\n",
    "        self.memory = np.zeros((memory_capacity, (state_dim + action_state_dim) * 2 + action_dim + 1)) # (s,a,r,s')\n",
    "        self.pointer = 0\n",
    "        # 定义 Actor 网络\n",
    "        self.actor = Actor(action_state_dim, action_dim)\n",
    "        self.actor_target = Actor(action_state_dim, action_dim)\n",
    "        # 定义 Critic 网络\n",
    "        self.critic = Critic(state_dim,action_dim)\n",
    "        self.critic_target = Critic(state_dim,action_dim)\n",
    "        # 定义优化器\n",
    "        self.aopt = torch.optim.Adam(self.actor.parameters(), lr=lr_a)\n",
    "        self.copt = torch.optim.Adam(self.critic.parameters(), lr=lr_c)\n",
    "        # 选取损失函数\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def sample(self):\n",
    "        indices = np.random.choice(self.memory_capacity, size=self.batch_size)\n",
    "        return self.memory[indices, :] \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.FloatTensor(s)\n",
    "        action = self.actor(s)\n",
    "        return action.detach().numpy()\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        # soft replacement and hard replacement\n",
    "        # 用于更新target网络的参数\n",
    "        if self.replacement['name'] == 'soft':\n",
    "            # soft的意思是每次learn的时候更新部分参数\n",
    "            tau = self.replacement['tau']\n",
    "            a_layers = self.actor_target.named_children()\n",
    "            c_layers = self.critic_target.named_children()\n",
    "            for al in a_layers:\n",
    "                al[1].weight.data.mul_((1-tau))\n",
    "                al[1].weight.data.add_(tau * self.actor.state_dict()[al[0]+'.weight'])\n",
    "                if al[1].bias is not None:\n",
    "                    al[1].bias.data.mul_((1-tau))\n",
    "                    al[1].bias.data.add_(tau * self.actor.state_dict()[al[0]+'.bias'])\n",
    "            for cl in c_layers:\n",
    "                cl[1].weight.data.mul_((1-tau))\n",
    "                cl[1].weight.data.add_(tau * self.critic.state_dict()[cl[0]+'.weight'])\n",
    "                if cl[1].bias is not None:\n",
    "                    cl[1].bias.data.mul_((1-tau))\n",
    "                    cl[1].bias.data.add_(tau * self.critic.state_dict()[cl[0]+'.bias'])\n",
    "            \n",
    "        else:\n",
    "            # hard的意思是每隔一定的步数才更新全部参数\n",
    "            if self.t_replace_counter % self.replacement['rep_iter'] == 0:\n",
    "                self.t_replace_counter = 0\n",
    "                a_layers = self.actor_target.named_children()\n",
    "                c_layers = self.critic_target.named_children()\n",
    "                for al in a_layers:\n",
    "                    al[1].weight.data = self.actor.state_dict()[al[0]+'.weight']\n",
    "                    if al[1].bias is not None:\n",
    "                        al[1].bias.data = self.actor.state_dict()[al[0]+'.bias']\n",
    "                for cl in c_layers:\n",
    "                    cl[1].weight.data = self.critic.state_dict()[cl[0]+'.weight']\n",
    "                    if cl[1].bias is not None:\n",
    "                        cl[1].bias.data = self.critic.state_dict()[cl[0]+'.bias']\n",
    "            \n",
    "            self.t_replace_counter += 1\n",
    "\n",
    "        # 从记忆库中采样bacth data\n",
    "        bm = self.sample()\n",
    "        bs = torch.FloatTensor(bm[:, : self.state_dim])\n",
    "        bas = torch.FloatTensor(bm[:, self.state_dim: self.state_dim + self.action_state_dim])\n",
    "        ba = torch.FloatTensor(bm[:, self.state_dim + self.action_state_dim: self.state_dim + self.action_state_dim + self.action_dim])\n",
    "        br = torch.FloatTensor(bm[:, -self.state_dim - self.action_state_dim - 1: -self.state_dim - self.action_state_dim])\n",
    "        bs_ = torch.FloatTensor(bm[:,-self.state_dim - self.action_state_dim: -self.action_state_dim])\n",
    "        bas_ = torch.FloatTensor(bm[:,-self.action_state_dim:])\n",
    "        \n",
    "        # 训练Actor\n",
    "        a = self.actor(bas)\n",
    "        q = self.critic(bs, a)\n",
    "        a_loss = -torch.mean(q)\n",
    "        self.aopt.zero_grad()\n",
    "        a_loss.backward(retain_graph=True)\n",
    "        self.aopt.step()\n",
    "        \n",
    "        # 训练critic\n",
    "        a_ = self.actor_target(bas_)\n",
    "        q_ = self.critic_target(bs_, a_)\n",
    "        q_target = br + self.gamma * q_\n",
    "        q_eval = self.critic(bs, ba)\n",
    "        td_error = self.mse_loss(q_target,q_eval)\n",
    "        self.copt.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.copt.step()\n",
    "\n",
    "    def store_transition(self, s, sa, a, r, s_, sa_):\n",
    "        transition = np.hstack((s, sa, a, [r], s_, sa_))\n",
    "        index = self.pointer % self.memory_capacity\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e56c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, T, a, b, r, p, c, W_0):\n",
    "        self.t = 0\n",
    "        self.W = W_0\n",
    "        self.W_0 = W_0\n",
    "        self.T = T\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.r = r\n",
    "        self.p = p\n",
    "        self.c = c\n",
    "        self.done = False\n",
    "        \n",
    "    def utility(self, W):\n",
    "        return -np.exp(-self.c * W)/self.c\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.W = self.W_0\n",
    "        self.done = False\n",
    "        return [self.t, self.W]\n",
    "    \n",
    "    def step(self, x):\n",
    "        W_new = x[0] * (np.random.choice((self.a, self.b), p=(self.p, 1-self.p)) - self.r) + self.W * (1 + self.r)\n",
    "        if self.t == 0:\n",
    "            reward = self.utility(self.W)\n",
    "        else:\n",
    "            reward = self.utility(W_new) - self.utility(self.W)\n",
    "        self.t += 1\n",
    "        self.W = W_new\n",
    "        if self.t > self.T - 1:\n",
    "            self.done = True\n",
    "        return [self.t, self.W], reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffde6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select(s, is_actor = False, T = 10, r = 0.1):\n",
    "    if is_actor:\n",
    "        return [1]\n",
    "    else:\n",
    "        return [T - s[0], np.power(1+r, T-s[0]) * s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a582534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 T: 9  Total Reward: -0.15352174227194915 Last investment: [1.1497867] Explore: 5\n",
      "Episode: 20 T: 9  Total Reward: -0.4295474040198248 Last investment: [8.98328255] Explore: 4.950224401048741\n",
      "Episode: 40 T: 9  Total Reward: -0.10772976650732793 Last investment: [4.18822822] Explore: 4.052495411575132\n",
      "Episode: 60 T: 9  Total Reward: -0.03926736319733007 Last investment: [2.00760447] Explore: 3.317570625153519\n",
      "Episode: 80 T: 9  Total Reward: -0.14927372455422233 Last investment: [3.91309843] Explore: 2.7159253089946316\n",
      "Episode: 100 T: 9  Total Reward: -0.15974435533910147 Last investment: [4.3279741] Explore: 2.2233890751598597\n",
      "Episode: 120 T: 9  Total Reward: -0.11404649690882165 Last investment: [2.8227005] Explore: 1.8201748638552033\n",
      "Episode: 140 T: 9  Total Reward: -0.0752107561987436 Last investment: [4.32199896] Explore: 1.4900840217414968\n",
      "Episode: 160 T: 9  Total Reward: -0.2469661200204224 Last investment: [3.00593281] Explore: 1.219855540223493\n",
      "Episode: 180 T: 9  Total Reward: -0.06756668817801391 Last investment: [4.75876744] Explore: 0.9986333101369902\n",
      "Episode: 200 T: 9  Total Reward: -0.09831261813119373 Last investment: [6.01514188] Explore: 0.8175299904220219\n",
      "Episode: 220 T: 9  Total Reward: -0.11645355739842245 Last investment: [4.22757724] Explore: 0.669269969722669\n",
      "Episode: 240 T: 9  Total Reward: -0.09205787621219058 Last investment: [3.84142721] Explore: 0.5478970773186645\n",
      "Episode: 260 T: 9  Total Reward: -0.08873080352521276 Last investment: [4.3351123] Explore: 0.44853530102169015\n",
      "Episode: 280 T: 9  Total Reward: 0.002875505232707763 Last investment: [4.28203503] Explore: 0.3671928991612542\n",
      "Episode: 300 T: 9  Total Reward: -0.15336856030506982 Last investment: [5.24490535] Explore: 0.30060203709122707\n",
      "Episode: 320 T: 9  Total Reward: 0.013325862080617833 Last investment: [4.93893791] Explore: 0.246087505803627\n",
      "Episode: 340 T: 9  Total Reward: -0.11837293892262137 Last investment: [4.74760157] Explore: 0.20145924857545627\n",
      "Episode: 360 T: 9  Total Reward: -0.08790878278884393 Last investment: [5.2550323] Explore: 0.16492437803394275\n",
      "Episode: 380 T: 9  Total Reward: -0.10420472169317767 Last investment: [5.03049269] Explore: 0.1350151490299793\n",
      "Episode: 400 T: 9  Total Reward: -0.13786499507437344 Last investment: [5.3632714] Explore: 0.11052999371527615\n",
      "Episode: 420 T: 9  Total Reward: -0.0018122464991632073 Last investment: [5.39358375] Explore: 0.09048524997729182\n",
      "Episode: 440 T: 9  Total Reward: -0.0931030933714731 Last investment: [5.38218101] Explore: 0.07407564397899175\n",
      "Episode: 460 T: 9  Total Reward: -0.10426791125232732 Last investment: [5.49033537] Explore: 0.06064193923627771\n",
      "Episode: 480 T: 9  Total Reward: -0.12315297995623564 Last investment: [5.72190037] Explore: 0.04964445257309331\n",
      "Episode: 500 T: 9  Total Reward: -0.1062263201164404 Last investment: [5.79194147] Explore: 0.04064137298907047\n",
      "Episode: 520 T: 9  Total Reward: -0.0351383966995691 Last investment: [5.93928396] Explore: 0.03327101242590719\n",
      "Episode: 540 T: 9  Total Reward: -0.15584805461565956 Last investment: [6.10979164] Explore: 0.02723727537803809\n",
      "Episode: 560 T: 9  Total Reward: -0.12570632450429656 Last investment: [6.14203978] Explore: 0.022297763606418122\n",
      "Episode: 580 T: 9  Total Reward: -0.10005628467781943 Last investment: [6.22846535] Explore: 0.018254038076385513\n",
      "Episode: 600 T: 9  Total Reward: -0.030065861963457202 Last investment: [6.36448763] Explore: 0.014943646904491454\n",
      "Episode: 620 T: 9  Total Reward: -0.10975043132655847 Last investment: [6.46945801] Explore: 0.012233599046503972\n",
      "Episode: 640 T: 9  Total Reward: -0.10477671798019354 Last investment: [6.5731827] Explore: 0.01001502153973142\n",
      "Episode: 660 T: 9  Total Reward: -0.024808298436501702 Last investment: [6.68824953] Explore: 0.008198785660704457\n",
      "Episode: 680 T: 9  Total Reward: -0.10647480981008853 Last investment: [6.80504951] Explore: 0.006711926284281925\n",
      "Episode: 700 T: 9  Total Reward: -0.6280181877124824 Last investment: [6.91026097] Explore: 0.005494710596174285\n",
      "Episode: 720 T: 9  Total Reward: -0.11352574964994491 Last investment: [7.03782693] Explore: 0.0044982383978819426\n",
      "Episode: 740 T: 9  Total Reward: -0.1304796384819764 Last investment: [7.13732032] Explore: 0.0036824775991419066\n",
      "Episode: 760 T: 9  Total Reward: -0.13113313721920367 Last investment: [7.2692705] Explore: 0.0030146559761188153\n",
      "Episode: 780 T: 9  Total Reward: -0.11081614483086774 Last investment: [7.36831063] Explore: 0.0024679445861304416\n",
      "Episode: 800 T: 9  Total Reward: -0.11149533500374888 Last investment: [7.45645392] Explore: 0.0020203799466538197\n",
      "Episode: 820 T: 9  Total Reward: -0.012237259012916349 Last investment: [7.578999] Explore: 0.001653981678430259\n",
      "Episode: 840 T: 9  Total Reward: -0.11208409376211127 Last investment: [7.70079661] Explore: 0.0013540301650260406\n",
      "Episode: 860 T: 9  Total Reward: -0.13375608207257478 Last investment: [7.78594366] Explore: 0.0011084752096773302\n",
      "Episode: 880 T: 9  Total Reward: -0.11476649194192529 Last investment: [7.88204614] Explore: 0.0009074519329084314\n",
      "Episode: 900 T: 9  Total Reward: 0.06293367139249606 Last investment: [7.99896108] Explore: 0.0007428844626836121\n",
      "Episode: 920 T: 9  Total Reward: -0.12055532037535992 Last investment: [8.10199149] Explore: 0.0006081614958138044\n",
      "Episode: 940 T: 9  Total Reward: 0.06658128246386649 Last investment: [8.219185] Explore: 0.0004978706966819469\n",
      "Episode: 960 T: 9  Total Reward: -0.19168570686270756 Last investment: [8.32883065] Explore: 0.00040758126307038846\n",
      "Episode: 980 T: 9  Total Reward: 0.07036422340116266 Last investment: [8.44644176] Explore: 0.0003336659239299975\n",
      "Episode: 1000 T: 9  Total Reward: -0.4130206426543017 Last investment: [8.56040822] Explore: 0.00027315521806219994\n",
      "Episode: 1020 T: 9  Total Reward: -1.3182089335610074 Last investment: [8.67119296] Explore: 0.00022361819953260124\n",
      "Episode: 1040 T: 9  Total Reward: -0.12538787578328742 Last investment: [8.80361426] Explore: 0.00018306477729748398\n",
      "Episode: 1060 T: 9  Total Reward: -0.1261714946669039 Last investment: [8.91565565] Explore: 0.00014986576565335257\n",
      "Episode: 1080 T: 9  Total Reward: -0.2315791392742051 Last investment: [9.00144516] Explore: 0.0001226874336310367\n",
      "Episode: 1100 T: 9  Total Reward: -0.44439817108557533 Last investment: [9.1111476] Explore: 0.00010043792393378599\n",
      "Episode: 1120 T: 9  Total Reward: 0.08335164970259593 Last investment: [9.22626469] Explore: 8.22233888636581e-05\n",
      "Episode: 1140 T: 9  Total Reward: 0.07223471754952611 Last investment: [9.33856728] Explore: 6.731208104900032e-05\n",
      "Episode: 1160 T: 9  Total Reward: -0.12978291091488908 Last investment: [9.43139491] Explore: 5.510495636053515e-05\n",
      "Episode: 1180 T: 9  Total Reward: -0.13053205005722107 Last investment: [9.53908312] Explore: 4.511160802302344e-05\n",
      "Episode: 1200 T: 9  Total Reward: 0.0194606105926306 Last investment: [9.66724598] Explore: 3.693056510394715e-05\n",
      "Episode: 1220 T: 9  Total Reward: 0.021647412395395846 Last investment: [9.81154352] Explore: 3.0233163894330904e-05\n",
      "Episode: 1240 T: 9  Total Reward: -0.13020482466082722 Last investment: [9.91779352] Explore: 2.4750344233529765e-05\n",
      "Episode: 1260 T: 9  Total Reward: -0.13388221098570538 Last investment: [10.01181963] Explore: 2.026184033597244e-05\n",
      "Episode: 1280 T: 9  Total Reward: -0.14648191697573307 Last investment: [10.11343561] Explore: 1.658733187412686e-05\n",
      "Episode: 1300 T: 9  Total Reward: 0.10271928061162082 Last investment: [10.24431685] Explore: 1.3579199822927607e-05\n",
      "Episode: 1320 T: 9  Total Reward: -0.2861412940512799 Last investment: [10.34570411] Explore: 1.1116596040296178e-05\n",
      "Episode: 1340 T: 9  Total Reward: 0.10636553893866489 Last investment: [10.46919264] Explore: 9.100588336175311e-06\n",
      "Episode: 1360 T: 9  Total Reward: -0.1377098801726868 Last investment: [10.55202233] Explore: 7.450185988976844e-06\n",
      "Episode: 1380 T: 9  Total Reward: -0.13828594338107464 Last investment: [10.63435425] Explore: 6.09908603927402e-06\n",
      "Episode: 1400 T: 9  Total Reward: -0.2115130668221416 Last investment: [10.75782953] Explore: 4.9930096469411704e-06\n",
      "Episode: 1420 T: 9  Total Reward: -0.15107488005498343 Last investment: [10.92231759] Explore: 4.0875215030439254e-06\n",
      "Episode: 1440 T: 9  Total Reward: -0.21354832508309265 Last investment: [11.01736299] Explore: 3.3462446939356627e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1460 T: 9  Total Reward: -15.40187240485617 Last investment: [11.11916475] Explore: 2.739399301839526e-06\n",
      "Episode: 1480 T: 9  Total Reward: -0.14292070865609072 Last investment: [11.29397503] Explore: 2.24260603192552e-06\n",
      "Episode: 1500 T: 9  Total Reward: -0.14366566741223893 Last investment: [11.39746126] Explore: 1.8359068030175548e-06\n",
      "Episode: 1520 T: 9  Total Reward: -0.15451990476406582 Last investment: [11.5270976] Explore: 1.5029629553221862e-06\n",
      "Episode: 1540 T: 9  Total Reward: -0.14543122238531303 Last investment: [11.64726098] Explore: 1.2303988641242583e-06\n",
      "Episode: 1560 T: 9  Total Reward: -0.1465386371601038 Last investment: [11.81024027] Explore: 1.0072645899071656e-06\n",
      "Episode: 1580 T: 9  Total Reward: -0.14619752860542798 Last investment: [11.95436717] Explore: 8.245959775027788e-07\n",
      "Episode: 1600 T: 9  Total Reward: -0.14818921560658643 Last investment: [12.03838802] Explore: 6.750545317754409e-07\n",
      "Episode: 1620 T: 9  Total Reward: -0.14711311276991504 Last investment: [12.10349369] Explore: 5.526326022722129e-07\n",
      "Episode: 1640 T: 9  Total Reward: 0.13424599514434368 Last investment: [12.20976024] Explore: 4.524120329818794e-07\n",
      "Episode: 1660 T: 9  Total Reward: -0.22334711676755678 Last investment: [12.30635542] Explore: 3.7036658124266513e-07\n",
      "Episode: 1680 T: 9  Total Reward: -0.224155485088728 Last investment: [12.41162401] Explore: 3.032001682123116e-07\n",
      "Episode: 1700 T: 9  Total Reward: -0.15024780945404118 Last investment: [12.50272411] Explore: 2.482144628047347e-07\n",
      "Episode: 1720 T: 9  Total Reward: -0.15193824223823374 Last investment: [12.57869735] Explore: 2.0320047943476431e-07\n",
      "Episode: 1740 T: 9  Total Reward: -0.15545212995196017 Last investment: [13.11830253] Explore: 1.6634983463876743e-07\n",
      "Episode: 1760 T: 9  Total Reward: -5.578550622396895 Last investment: [13.3886726] Explore: 1.3618209741099148e-07\n",
      "Episode: 1780 T: 9  Total Reward: -0.49251925584374345 Last investment: [13.53248687] Explore: 1.1148531464145384e-07\n",
      "Episode: 1800 T: 9  Total Reward: -0.16622812718901384 Last investment: [13.54927659] Explore: 9.126732233528348e-08\n",
      "Episode: 1820 T: 9  Total Reward: -0.15872964567568926 Last investment: [13.55996039] Explore: 7.471588659942918e-08\n",
      "Episode: 1840 T: 9  Total Reward: -0.15875343271329095 Last investment: [13.5634371] Explore: 6.116607310808112e-08\n",
      "Episode: 1860 T: 9  Total Reward: -0.15784464920842017 Last investment: [13.57132652] Explore: 5.007353415373531e-08\n",
      "Episode: 1880 T: 9  Total Reward: -0.4985596578864081 Last investment: [13.60338468] Explore: 4.099264012281387e-08\n",
      "Episode: 1900 T: 9  Total Reward: -0.15974265449896558 Last investment: [13.71073201] Explore: 3.3558576853780504e-08\n",
      "Episode: 1920 T: 9  Total Reward: -0.15899020267064753 Last investment: [13.71925788] Explore: 2.7472689660316234e-08\n",
      "Episode: 1940 T: 9  Total Reward: 0.08765902119349417 Last investment: [13.72473024] Explore: 2.2490485233047712e-08\n",
      "Episode: 1960 T: 9  Total Reward: 0.1625556565053415 Last investment: [13.74015634] Explore: 1.841180941044104e-08\n",
      "Episode: 1980 T: 9  Total Reward: -0.15926592410850565 Last investment: [13.75864202] Explore: 1.507280622244128e-08\n",
      "Episode: 2000 T: 9  Total Reward: -0.1592089443101392 Last investment: [13.76450364] Explore: 1.2339335170959849e-08\n",
      "Episode: 2020 T: 9  Total Reward: 0.08850205948054032 Last investment: [13.77161025] Explore: 1.0101582294250843e-08\n",
      "Episode: 2040 T: 9  Total Reward: -0.15948278109658337 Last investment: [13.78974481] Explore: 8.269648521070554e-09\n",
      "Episode: 2060 T: 9  Total Reward: 0.1636802602593108 Last investment: [13.80178365] Explore: 6.769938081974133e-09\n",
      "Episode: 2080 T: 9  Total Reward: 0.15571130250472556 Last investment: [13.80890499] Explore: 5.542201886450962e-09\n",
      "Episode: 2100 T: 9  Total Reward: -0.1677667446617926 Last investment: [13.8164893] Explore: 4.537117087077371e-09\n",
      "Episode: 2120 T: 9  Total Reward: -0.16059267675567027 Last investment: [13.83453629] Explore: 3.7143055925434093e-09\n",
      "Episode: 2140 T: 9  Total Reward: 0.16378012324335198 Last investment: [13.85170156] Explore: 3.0407119256616127e-09\n",
      "Episode: 2160 T: 9  Total Reward: -0.160863578634318 Last investment: [13.87440161] Explore: 2.4892752587246065e-09\n",
      "Episode: 2180 T: 9  Total Reward: -0.16097844843919998 Last investment: [13.89157642] Explore: 2.0378422768050276e-09\n",
      "Episode: 2200 T: 9  Total Reward: 0.15382361695629013 Last investment: [13.68360086] Explore: 1.6682771945685134e-09\n",
      "Episode: 2220 T: 9  Total Reward: -0.15872385758818966 Last investment: [13.54624141] Explore: 1.3657331725794147e-09\n",
      "Episode: 2240 T: 9  Total Reward: -0.4850099568292512 Last investment: [13.44601718] Explore: 1.1180558631122831e-09\n",
      "Episode: 2260 T: 9  Total Reward: -0.16438207375416788 Last investment: [13.22796648] Explore: 9.152951236285961e-10\n",
      "Episode: 2280 T: 9  Total Reward: -0.1643546575707444 Last investment: [13.2236394] Explore: 7.493052815860527e-10\n",
      "Episode: 2300 T: 9  Total Reward: -0.16433218502615296 Last investment: [13.21895426] Explore: 6.134178916925817e-10\n",
      "Episode: 2320 T: 9  Total Reward: 0.15151964527333728 Last investment: [13.17483468] Explore: 5.021738390153846e-10\n",
      "Episode: 2340 T: 9  Total Reward: -0.15601335941450323 Last investment: [13.16253575] Explore: 4.111040255047372e-10\n",
      "Episode: 2360 T: 9  Total Reward: -0.1545268560665586 Last investment: [13.09003223] Explore: 3.3654982927340833e-10\n",
      "Episode: 2380 T: 9  Total Reward: -0.15289627323995938 Last investment: [12.88214423] Explore: 2.755161237959101e-10\n",
      "Episode: 2400 T: 9  Total Reward: 0.13788596901402267 Last investment: [12.8791549] Explore: 2.255509522480127e-10\n",
      "Episode: 2420 T: 9  Total Reward: 0.14699401164423856 Last investment: [12.86299619] Explore: 1.846470230456272e-10\n",
      "Episode: 2440 T: 9  Total Reward: 0.14535099825037454 Last investment: [12.83184052] Explore: 1.5116106928301746e-10\n",
      "Episode: 2460 T: 9  Total Reward: -0.15053530326355938 Last investment: [12.53268502] Explore: 1.237478324312812e-10\n",
      "Episode: 2480 T: 9  Total Reward: -0.1602603973368396 Last investment: [12.51447504] Explore: 1.0130601817038672e-10\n",
      "Episode: 2500 T: 9  Total Reward: 0.06632497693872247 Last investment: [12.51429298] Explore: 8.293405319432842e-11\n",
      "Episode: 2520 T: 9  Total Reward: -0.1602542805431171 Last investment: [12.51335231] Explore: 6.789386557145581e-11\n",
      "Episode: 2540 T: 9  Total Reward: -0.15033998534288945 Last investment: [12.51269774] Explore: 5.5581233578852105e-11\n",
      "Episode: 2560 T: 9  Total Reward: -0.1515179468028549 Last investment: [12.51253301] Explore: 4.5501511810305906e-11\n",
      "Episode: 2580 T: 9  Total Reward: -0.15033760642441438 Last investment: [12.51237783] Explore: 3.724975938301522e-11\n",
      "Episode: 2600 T: 9  Total Reward: -0.15017620190699094 Last investment: [12.51209259] Explore: 3.0494471917266235e-11\n",
      "Episode: 2620 T: 9  Total Reward: -0.16019949261872488 Last investment: [12.49261249] Explore: 2.496426374063912e-11\n",
      "Episode: 2640 T: 9  Total Reward: -0.14896983412352863 Last investment: [12.34827475] Explore: 2.043696529007016e-11\n",
      "Episode: 2660 T: 9  Total Reward: -0.15036730557752068 Last investment: [12.34719103] Explore: 1.6730697712811447e-11\n",
      "Episode: 2680 T: 9  Total Reward: -0.1503662047252755 Last investment: [12.34704451] Explore: 1.3696566098953967e-11\n",
      "Episode: 2700 T: 9  Total Reward: -0.15929334488155766 Last investment: [12.34687458] Explore: 1.121267780478542e-11\n",
      "Episode: 2720 T: 9  Total Reward: -0.15036302083110076 Last investment: [12.34658761] Explore: 9.17924556020866e-12\n",
      "Episode: 2740 T: 9  Total Reward: -0.22366425937626253 Last investment: [12.3464489] Explore: 7.514578633361782e-12\n",
      "Episode: 2760 T: 9  Total Reward: -0.14911862088853234 Last investment: [12.34574145] Explore: 6.1518010022267874e-12\n",
      "Episode: 2780 T: 9  Total Reward: -3.745704976321261 Last investment: [12.34553077] Explore: 5.036164689658454e-12\n",
      "Episode: 2800 T: 9  Total Reward: -0.15034611811038626 Last investment: [12.34408812] Explore: 4.122850328250529e-12\n",
      "Episode: 2820 T: 9  Total Reward: -0.15034373744889148 Last investment: [12.34380982] Explore: 3.3751665953379043e-12\n",
      "Episode: 2840 T: 9  Total Reward: -0.15927175903409718 Last investment: [12.34305295] Explore: 2.7630761825687682e-12\n",
      "Episode: 2860 T: 9  Total Reward: 0.12769585334497596 Last investment: [12.34037919] Explore: 2.26198908262022e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2880 T: 9  Total Reward: 0.13659239532072928 Last investment: [12.33822389] Explore: 1.8517747147804963e-12\n",
      "Episode: 2900 T: 9  Total Reward: -0.14905545015018748 Last investment: [12.3371237] Explore: 1.5159532027131892e-12\n",
      "Episode: 2920 T: 9  Total Reward: -0.14887947503890728 Last investment: [12.33664946] Explore: 1.2410333149455402e-12\n",
      "Episode: 2940 T: 9  Total Reward: -27.729513719747786 Last investment: [12.3358215] Explore: 1.0159704706241572e-12\n",
      "Episode: 2960 T: 9  Total Reward: -0.15916660212390113 Last investment: [12.32449705] Explore: 8.317230365613247e-13\n",
      "Episode: 2980 T: 9  Total Reward: -0.1591442140122943 Last investment: [12.32085922] Explore: 6.80889090331345e-13\n",
      "Episode: 3000 T: 9  Total Reward: -0.398703947243795 Last investment: [12.31775024] Explore: 5.574090568045289e-13\n",
      "Episode: 3020 T: 9  Total Reward: -0.14890761031093788 Last investment: [12.31695088] Explore: 4.563222718938177e-13\n",
      "Episode: 3040 T: 9  Total Reward: -3.7101831518366364 Last investment: [12.31639082] Explore: 3.7356769375090556e-13\n",
      "Episode: 3060 T: 9  Total Reward: 0.1361296004850207 Last investment: [12.31162158] Explore: 3.058207552202122e-13\n",
      "Episode: 3080 T: 9  Total Reward: -0.6842568045065984 Last investment: [12.30884899] Explore: 2.503598032912991e-13\n",
      "Episode: 3100 T: 9  Total Reward: 0.1269596025578026 Last investment: [12.30130889] Explore: 2.049567599129235e-13\n",
      "Episode: 3120 T: 9  Total Reward: -0.15000286373325983 Last investment: [12.29457422] Explore: 1.6778761159644853e-13\n",
      "Episode: 3140 T: 9  Total Reward: 0.06236625567447948 Last investment: [12.28375695] Explore: 1.373591318344486e-13\n",
      "Episode: 3160 T: 9  Total Reward: -0.15871958671105435 Last investment: [12.24477941] Explore: 1.1244889249447291e-13\n",
      "Episode: 3180 T: 9  Total Reward: -0.1481745797279479 Last investment: [12.21722083] Explore: 9.205615421676913e-14\n",
      "Episode: 3200 T: 9  Total Reward: -0.22269533467838032 Last investment: [12.21634778] Explore: 7.536166289586283e-14\n",
      "Episode: 3220 T: 9  Total Reward: -0.14944446093302297 Last investment: [12.21474734] Explore: 6.169473711726161e-14\n",
      "Episode: 3240 T: 9  Total Reward: -0.14937165570419125 Last investment: [12.20351132] Explore: 5.050632432603835e-14\n",
      "Episode: 3260 T: 9  Total Reward: 0.1249062321607964 Last investment: [12.19183575] Explore: 4.134694329077964e-14\n",
      "Episode: 3280 T: 9  Total Reward: 0.06070775905836945 Last investment: [12.18697288] Explore: 3.384862672751631e-14\n",
      "Episode: 3300 T: 9  Total Reward: -0.14768981689601932 Last investment: [12.17623104] Explore: 2.7710138649940486e-14\n",
      "Episode: 3320 T: 9  Total Reward: -0.14915106008472986 Last investment: [12.17194644] Explore: 2.2684872570464477e-14\n",
      "Episode: 3340 T: 9  Total Reward: -0.1477195281672242 Last investment: [12.15527448] Explore: 1.857094437668276e-14\n",
      "Episode: 3360 T: 9  Total Reward: -0.14900013723155942 Last investment: [12.15098554] Explore: 1.5203081876284195e-14\n",
      "Episode: 3380 T: 9  Total Reward: -0.22190638965822962 Last investment: [12.10891117] Explore: 1.2445985182487908e-14\n",
      "Episode: 3400 T: 9  Total Reward: 0.13255664949728058 Last investment: [12.03063705] Explore: 1.0188891201352161e-14\n",
      "Episode: 3420 T: 9  Total Reward: -0.15742594590339376 Last investment: [12.02270508] Explore: 8.341123855672107e-15\n",
      "Episode: 3440 T: 9  Total Reward: -0.2212011410287924 Last investment: [12.01672901] Explore: 6.828451280982297e-15\n",
      "Episode: 3460 T: 9  Total Reward: -0.14792063788259505 Last investment: [11.99460723] Explore: 5.590103648328051e-15\n",
      "Episode: 3480 T: 9  Total Reward: -0.14577789222234164 Last investment: [11.85899214] Explore: 4.576331808368005e-15\n",
      "Episode: 3500 T: 9  Total Reward: -0.15370036098742118 Last investment: [11.36715629] Explore: 3.746408678226308e-15\n",
      "Episode: 3520 T: 9  Total Reward: 0.11891709331712107 Last investment: [11.32272807] Explore: 3.066993079178567e-15\n",
      "Episode: 3540 T: 9  Total Reward: -0.14000380740251522 Last investment: [11.10340552] Explore: 2.5107902942886087e-15\n",
      "Episode: 3560 T: 9  Total Reward: 0.1106679900745713 Last investment: [10.8417511] Explore: 2.0554555354856887e-15\n",
      "Episode: 3580 T: 9  Total Reward: 0.03809973785345287 Last investment: [10.83212506] Explore: 1.6826962681707422e-15\n",
      "Episode: 3600 T: 9  Total Reward: -0.13852167923968423 Last investment: [10.65646952] Explore: 1.377537330306046e-15\n",
      "Episode: 3620 T: 9  Total Reward: -0.138225217997226 Last investment: [10.61993252] Explore: 1.1277193230181684e-15\n",
      "Episode: 3640 T: 9  Total Reward: -0.14923436134938858 Last investment: [10.59337182] Explore: 9.232061037692626e-16\n",
      "Episode: 3660 T: 9  Total Reward: -0.14916265232208864 Last investment: [10.580994] Explore: 7.557815962182393e-16\n",
      "Episode: 3680 T: 9  Total Reward: -0.13577487317843168 Last investment: [10.56772492] Explore: 6.187197190855567e-16\n",
      "Episode: 3700 T: 9  Total Reward: -0.1377254364257639 Last investment: [10.55000305] Explore: 5.065141738047416e-16\n",
      "Episode: 3720 T: 9  Total Reward: 0.10555917373287382 Last investment: [10.54158644] Explore: 4.146572354995901e-16\n",
      "Episode: 3740 T: 9  Total Reward: 0.1051376793009195 Last investment: [10.51419952] Explore: 3.394586604765864e-16\n",
      "Episode: 3760 T: 9  Total Reward: 0.10426423492186654 Last investment: [10.46517025] Explore: 2.7789743505554306e-16\n",
      "Episode: 3780 T: 9  Total Reward: -0.13609749229197124 Last investment: [10.31506972] Explore: 2.2750040992333527e-16\n",
      "Episode: 3800 T: 9  Total Reward: -0.20775156183421353 Last investment: [10.27883009] Explore: 1.86242944289648e-16\n",
      "Episode: 3820 T: 9  Total Reward: -0.13571500270097492 Last investment: [10.26622599] Explore: 1.5246756834137506e-16\n",
      "Episode: 3840 T: 9  Total Reward: -0.2075861671296661 Last investment: [10.25769754] Explore: 1.248173963561207e-16\n",
      "Episode: 3860 T: 9  Total Reward: -0.20747479647429637 Last investment: [10.24429495] Explore: 1.0218161542550916e-16\n",
      "Episode: 3880 T: 9  Total Reward: -0.1326052572096614 Last investment: [10.15297716] Explore: 8.36508598623292e-17\n",
      "Episode: 3900 T: 9  Total Reward: -0.13231432610831173 Last investment: [10.11844288] Explore: 6.848067851117715e-17\n",
      "Episode: 3920 T: 9  Total Reward: -0.2754353742267029 Last investment: [10.1055232] Explore: 5.606162730507791e-17\n",
      "Episode: 3940 T: 9  Total Reward: 0.023564683496205066 Last investment: [9.9217753] Explore: 4.589478557196957e-17\n",
      "Episode: 3960 T: 9  Total Reward: -0.1327772118749073 Last investment: [9.84997489] Explore: 3.7571712487665845e-17\n",
      "Episode: 3980 T: 9  Total Reward: -0.13255919404012087 Last investment: [9.8195102] Explore: 3.075803844953548e-17\n",
      "Episode: 4000 T: 9  Total Reward: -0.12927191755055845 Last investment: [9.79311856] Explore: 2.5180032173771103e-17\n",
      "Episode: 4020 T: 9  Total Reward: -0.20329420190606762 Last investment: [9.72245737] Explore: 2.0613603865292103e-17\n",
      "Episode: 4040 T: 9  Total Reward: -0.131736593320523 Last investment: [9.70355034] Explore: 1.687530267565766e-17\n",
      "Episode: 4060 T: 9  Total Reward: 0.019888318019117807 Last investment: [9.69217734] Explore: 1.3814946782524837e-17\n",
      "Episode: 4080 T: 9  Total Reward: 0.09085773144287865 Last investment: [9.66952064] Explore: 1.1309590012823602e-17\n",
      "Episode: 4100 T: 9  Total Reward: -0.1287465189540616 Last investment: [9.65811469] Explore: 9.258582625881307e-18\n",
      "Episode: 4120 T: 9  Total Reward: -0.1285964644175031 Last investment: [9.63925535] Explore: 7.579527829308959e-18\n",
      "Episode: 4140 T: 9  Total Reward: -0.1309394181651742 Last investment: [9.58862218] Explore: 6.2049715854645185e-18\n",
      "Episode: 4160 T: 9  Total Reward: 0.08926693700314603 Last investment: [9.57473842] Explore: 5.079692725388721e-18\n",
      "Episode: 4180 T: 9  Total Reward: 0.0764108797871523 Last investment: [9.56314867] Explore: 4.158484503750616e-18\n",
      "Episode: 4200 T: 9  Total Reward: -0.12719417566617808 Last investment: [9.53636516] Explore: 3.404338471400489e-18\n",
      "Episode: 4220 T: 9  Total Reward: -0.12710954931798232 Last investment: [9.4354066] Explore: 2.7869577047610982e-18\n",
      "Episode: 4240 T: 9  Total Reward: -0.1290743395088399 Last investment: [9.32568117] Explore: 2.281539662809138e-18\n",
      "Episode: 4260 T: 9  Total Reward: -0.14205969005403765 Last investment: [9.31693944] Explore: 1.8677797743677832e-18\n",
      "Episode: 4280 T: 9  Total Reward: -0.1253232453532918 Last investment: [9.30548841] Explore: 1.5290557260100558e-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4300 T: 9  Total Reward: 0.07156907398694026 Last investment: [9.29765181] Explore: 1.2517596803057375e-18\n",
      "Episode: 4320 T: 9  Total Reward: -0.14188797434919634 Last investment: [9.28654237] Explore: 1.0247515970708437e-18\n",
      "Episode: 4340 T: 9  Total Reward: -0.4542659627198409 Last investment: [9.27049637] Explore: 8.389116954484093e-19\n",
      "Episode: 4360 T: 9  Total Reward: -0.12555894577423293 Last investment: [9.24999237] Explore: 6.867740775147779e-19\n",
      "Episode: 4380 T: 9  Total Reward: 0.08365330186349523 Last investment: [9.23835494] Explore: 5.622267946737428e-19\n",
      "Episode: 4400 T: 9  Total Reward: -0.14154023455169606 Last investment: [9.22311609] Explore: 4.602663073611849e-19\n",
      "Episode: 4420 T: 9  Total Reward: 0.08623006932264822 Last investment: [9.2119451] Explore: 3.7679647376968794e-19\n",
      "Episode: 4440 T: 9  Total Reward: -1.3213738719123338 Last investment: [9.17360479] Explore: 3.0846399220323227e-19\n",
      "Episode: 4460 T: 9  Total Reward: -0.23704312328438704 Last investment: [9.15178559] Explore: 2.5252368615348333e-19\n",
      "Episode: 4480 T: 9  Total Reward: 0.08183779406635656 Last investment: [9.12848126] Explore: 2.0672822008517934e-19\n",
      "Episode: 4500 T: 9  Total Reward: -0.14084901520072124 Last investment: [9.09794981] Explore: 1.6923781539293387e-19\n",
      "Episode: 4520 T: 9  Total Reward: 0.009722301647753923 Last investment: [9.03859832] Explore: 1.3854633947494674e-19\n",
      "Episode: 4540 T: 9  Total Reward: -0.12270414128902442 Last investment: [8.98057851] Explore: 1.1342079863971491e-19\n",
      "Episode: 4560 T: 9  Total Reward: -0.13956388016092564 Last investment: [8.8593613] Explore: 9.285180404493463e-20\n",
      "Episode: 4580 T: 9  Total Reward: 0.0615855644013327 Last investment: [8.74507037] Explore: 7.601302069636543e-20\n",
      "Episode: 4600 T: 9  Total Reward: -0.11998616781203768 Last investment: [8.53388786] Explore: 6.222797041821482e-20\n",
      "Episode: 4620 T: 9  Total Reward: -0.1372037284616496 Last investment: [8.42383038] Explore: 5.094285514370273e-20\n",
      "Episode: 4640 T: 9  Total Reward: 0.06825643026446873 Last investment: [8.3140746] Explore: 4.1704308733691925e-20\n",
      "Episode: 4660 T: 9  Total Reward: -0.11702972369821814 Last investment: [8.29402924] Explore: 3.414118352905252e-20\n",
      "Episode: 4680 T: 9  Total Reward: -0.11787482467644914 Last investment: [8.27039892] Explore: 2.794963993307413e-20\n",
      "Episode: 4700 T: 9  Total Reward: -0.11762944621664245 Last investment: [8.24004173] Explore: 2.288094001556044e-20\n",
      "Episode: 4720 T: 9  Total Reward: -0.11641854160114767 Last investment: [8.21993394] Explore: 1.873145476110942e-20\n",
      "Episode: 4740 T: 9  Total Reward: -0.003338778074987267 Last investment: [8.17339637] Explore: 1.5334483514614247e-20\n",
      "Episode: 4760 T: 9  Total Reward: -0.13552409231924578 Last investment: [8.1118523] Explore: 1.2553556979898387e-20\n",
      "Episode: 4780 T: 9  Total Reward: -0.12039260755331484 Last investment: [8.06946321] Explore: 1.027695472738717e-20\n",
      "Episode: 4800 T: 9  Total Reward: 0.0489325938730985 Last investment: [8.04990075] Explore: 8.413216958180441e-21\n",
      "Episode: 4820 T: 9  Total Reward: 0.06335796014660842 Last investment: [8.01948287] Explore: 6.88747021496422e-21\n",
      "Episode: 4840 T: 9  Total Reward: -0.11575063576952056 Last investment: [8.00085068] Explore: 5.6384194295494176e-21\n",
      "Episode: 4860 T: 9  Total Reward: 0.06284101857390503 Last investment: [7.98916383] Explore: 4.615885466110225e-21\n",
      "Episode: 4880 T: 9  Total Reward: -0.11547215927637626 Last investment: [7.96452002] Explore: 3.778789233838582e-21\n",
      "Episode: 4900 T: 9  Total Reward: -0.11410149302423128 Last investment: [7.94145324] Explore: 3.0935013831284242e-21\n",
      "Episode: 4920 T: 9  Total Reward: -0.11458627585327423 Last investment: [7.84630429] Explore: 2.5324912862886264e-21\n",
      "Episode: 4940 T: 9  Total Reward: -0.1142166071412694 Last investment: [7.80404004] Explore: 2.073221027185031e-21\n",
      "Episode: 4960 T: 9  Total Reward: -0.11807372616082161 Last investment: [7.71811572] Explore: 1.6972399671555187e-21\n",
      "Episode: 4980 T: 9  Total Reward: -0.8019933052576946 Last investment: [7.65230092] Explore: 1.3894435124562231e-21\n",
      "Running time:  80.6530122756958\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # hyper parameters\n",
    "    VAR = 5  # control exploration\n",
    "    MAX_EPISODES = 5000\n",
    "    MAX_EP_STEPS = 20\n",
    "    MEMORY_CAPACITY = 200\n",
    "    REPLACEMENT = [\n",
    "        dict(name='soft', tau=0.05),\n",
    "        dict(name='hard', rep_iter=600)\n",
    "    ][0]  # you can try different target replacement strategies\n",
    "    \n",
    "    T = 10\n",
    "    a = 0.18\n",
    "    b = 0.02\n",
    "    r = 0.1\n",
    "    p = 2./3.\n",
    "    c = 1.\n",
    "    W_0 = 1.\n",
    "    \n",
    "    # train\n",
    "    env = Env(T, a, b, r, p, c, W_0)\n",
    "\n",
    "    s_dim = 2\n",
    "    as_dim = 1\n",
    "    a_dim = 1\n",
    "    ddpg = DDPG(state_dim=s_dim,\n",
    "                action_state_dim = as_dim,\n",
    "                action_dim=a_dim,\n",
    "                replacement=REPLACEMENT,\n",
    "                memory_capacity=MEMORY_CAPACITY)\n",
    "\n",
    "    t1 = time.time()\n",
    "    for i in range(MAX_EPISODES):\n",
    "        s = env.reset()\n",
    "        ep_reward = 0\n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            # Add exploration noise\n",
    "            s_s = feature_select(s, is_actor = False, T = env.T, r = env.r)\n",
    "            a_s = feature_select(s, is_actor = True, T = env.T, r = env.r)\n",
    "            a = ddpg.choose_action(a_s)\n",
    "            a = np.random.normal(a, VAR)  # 在动作选择上添加随机噪声\n",
    "            x = a * np.power(1 + env.r, env.t - env.T)\n",
    "\n",
    "            s_, reward, done = env.step(x)\n",
    "            \n",
    "            s_s_ = feature_select(s_, is_actor = False, T = env.T, r = env.r)\n",
    "            a_s_ = feature_select(s_, is_actor = True, T = env.T, r = env.r)\n",
    "            ddpg.store_transition(s_s, a_s, a, reward, s_s_, a_s_)\n",
    "\n",
    "            if ddpg.pointer > MEMORY_CAPACITY:\n",
    "                VAR *= .99999  # decay the action randomness\n",
    "                ddpg.learn()\n",
    "\n",
    "            s = s_\n",
    "            ep_reward += reward\n",
    "            if done or j == MAX_EP_STEPS - 1:\n",
    "                if i % 20 == 0:\n",
    "                    print('Episode:', i, 'T:', j, ' Total Reward:', ep_reward, 'Last investment:', x,'Explore:' , VAR, )\n",
    "                break\n",
    "\n",
    "    print('Running time: ', time.time() - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
